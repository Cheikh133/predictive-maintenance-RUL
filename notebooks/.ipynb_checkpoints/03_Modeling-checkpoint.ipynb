{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470ca5aa-9fae-4383-b7be-e8df89806358",
   "metadata": {},
   "source": [
    "# 03 – Modeling\n",
    "\n",
    "Load feature-engineered data, train and compare regression models for RUL prediction, and prepare the chosen pipeline for production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06446d44-e035-4e29-a832-170fd123fa2d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries and set a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26ca757d-927e-4df7-8106-fbe1e19a28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV, GroupKFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import joblib\n",
    "import optuna\n",
    "\n",
    "\n",
    "# Fix random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e26964-f115-4f4a-b27e-a139e8221b05",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load processed train and test sets, define feature matrix (X) and target vectors (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "388b1a53-1130-4729-be56-6c1242be41fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20631, 28)\n",
      "y_train shape: (20631,)\n",
      "X_test shape:  (100, 28)\n",
      "y_test shape:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# Load processed feature datasets\n",
    "train = pd.read_csv(\"../data/processed/train_features.csv\")\n",
    "test  = pd.read_csv(\"../data/processed/test_features.csv\")\n",
    "\n",
    "# Define feature columns (exclude identifiers and targets)\n",
    "feature_cols = [c for c in train.columns if c not in [\"unit_number\", \"time_in_cycles\", \"RUL\"]]\n",
    "\n",
    "# Prepare train and test matrices\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"RUL\"]\n",
    "\n",
    "X_test  = test[feature_cols]\n",
    "y_test  = test[\"true_RUL\"]\n",
    "\n",
    "# Display shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49209a8a-3331-4b11-9243-9f09971bac3e",
   "metadata": {},
   "source": [
    "## 3. Define Metrics and CV\n",
    "\n",
    "We use 5-fold `GroupKFold` on `unit_number` to avoid leaking engine-specific patterns.  \n",
    "Metrics:  \n",
    "- **MAE** (Mean Absolute Error)  \n",
    "- **RMSE** (Root Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d8304fb-32b4-4468-a20a-e69330be8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Define scorers (negative because sklearn expects higher = better)\n",
    "mae_scorer  = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred)),\n",
    "                           greater_is_better=False)\n",
    "\n",
    "# CV splitter\n",
    "groups = train[\"unit_number\"]\n",
    "cv = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e154c0-d6b4-4a19-9ae2-5bee940facf5",
   "metadata": {},
   "source": [
    "## 4. Baseline Models\n",
    "\n",
    "Train and evaluate simple regression baselines with GroupKFold CV:\n",
    "- LinearRegression  \n",
    "- Ridge (α=1.0)  \n",
    "- Lasso (α=0.1)  \n",
    "\n",
    "We record MAE and RMSE for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df559a48-2481-4b6a-9692-0045c4d285e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              model   MAE_mean   MAE_std  RMSE_mean  RMSE_std\n",
      "0  LinearRegression  33.867099  2.050906  44.237825  3.451820\n",
      "1             Ridge  33.866912  2.050889  44.237566  3.451892\n",
      "2             Lasso  33.867179  2.042821  44.247170  3.454029\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
    "    \"Lasso\": Lasso(alpha=0.1, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    mae_scores = cross_val_score(model, X_train, y_train,\n",
    "                                 scoring=mae_scorer, cv=cv, groups=groups)\n",
    "    rmse_scores = cross_val_score(model, X_train, y_train,\n",
    "                                  scoring=rmse_scorer, cv=cv, groups=groups)\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"MAE_mean\": -mae_scores.mean(),\n",
    "        \"MAE_std\":  mae_scores.std(),\n",
    "        \"RMSE_mean\": -rmse_scores.mean(),\n",
    "        \"RMSE_std\": rmse_scores.std()\n",
    "    })\n",
    "\n",
    "cv_baseline = pd.DataFrame(results)\n",
    "print(cv_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2411ee-e8e7-4eda-a502-9ea4d1b5deb5",
   "metadata": {},
   "source": [
    "The baseline linear models perform almost identically:\n",
    "- Ridge has the lowest MAE (33.8669) and RMSE (44.2376), slightly outperforming LinearRegression.\n",
    "- Lasso shows comparable MAE (33.8672) but a marginally higher RMSE (44.2472).\n",
    "\n",
    "Regularization at these α values has minimal effect; Ridge offers the best baseline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b504567-175f-4be4-95b0-1ee5abd6d6e6",
   "metadata": {},
   "source": [
    "## 5. Tree Ensemble Models\n",
    "\n",
    "Next, we train and evaluate:\n",
    "- RandomForestRegressor (n_estimators=100)  \n",
    "- LightGBMRegressor (default parameters)\n",
    "\n",
    "We use the same GroupKFold CV and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "748ce8ad-95e6-45e5-98ab-97976c161084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5496\n",
      "[LightGBM] [Info] Number of data points in the train set: 16511, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.636727\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5500\n",
      "[LightGBM] [Info] Number of data points in the train set: 16498, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.750091\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5496\n",
      "[LightGBM] [Info] Number of data points in the train set: 16505, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.801272\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5500\n",
      "[LightGBM] [Info] Number of data points in the train set: 16506, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.898946\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5496\n",
      "[LightGBM] [Info] Number of data points in the train set: 16504, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.952315\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5496\n",
      "[LightGBM] [Info] Number of data points in the train set: 16511, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.636727\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5500\n",
      "[LightGBM] [Info] Number of data points in the train set: 16498, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.750091\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5496\n",
      "[LightGBM] [Info] Number of data points in the train set: 16505, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.801272\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5500\n",
      "[LightGBM] [Info] Number of data points in the train set: 16506, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.898946\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5496\n",
      "[LightGBM] [Info] Number of data points in the train set: 16504, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 107.952315\n",
      "          model   MAE_mean   MAE_std  RMSE_mean  RMSE_std\n",
      "0  RandomForest  30.119928  2.396733  42.883562  3.310426\n",
      "1      LightGBM  30.280295  2.390070  42.897641  3.516611\n"
     ]
    }
   ],
   "source": [
    "ensemble_models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in ensemble_models.items():\n",
    "    mae_scores = cross_val_score(model, X_train, y_train,\n",
    "                                 scoring=mae_scorer, cv=cv, groups=groups)\n",
    "    rmse_scores = cross_val_score(model, X_train, y_train,\n",
    "                                  scoring=rmse_scorer, cv=cv, groups=groups)\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"MAE_mean\": -mae_scores.mean(),\n",
    "        \"MAE_std\":  mae_scores.std(),\n",
    "        \"RMSE_mean\": -rmse_scores.mean(),\n",
    "        \"RMSE_std\": rmse_scores.std()\n",
    "    })\n",
    "\n",
    "cv_ensemble = pd.DataFrame(results)\n",
    "print(cv_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c34f5-64f1-42f0-bb04-834920ec1574",
   "metadata": {},
   "source": [
    "Ridge (MAE ≃ 33.87, RMSE ≃ 44.24) was our best linear baseline.  \n",
    "Ensembles reduce MAE by ~3.6 cycles:\n",
    "- RandomForest: MAE ≃ 30.12 (σ ≃ 2.40), RMSE ≃ 42.88 (σ ≃ 3.31)  \n",
    "- LightGBM:     MAE ≃ 30.28 (σ ≃ 2.39), RMSE ≃ 42.90 (σ ≃ 3.52)\n",
    "\n",
    "RandomForest and LightGBM are our leading candidates for hyperparameter tuning, with RandomForest slightly ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90107bdd-ed2b-4400-912e-4f39943423a8",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "We optimize RandomForest and LightGBM efficiently, using:\n",
    "\n",
    "- **RandomizedSearchCV** on RandomForest with a limited number of iterations and 3-fold GroupKFold to speed up tuning.  \n",
    "- **Optuna + LightGBM’s native CV** with early stopping to find good parameters without exhaustive grid search.\n",
    "\n",
    "Both optimize MAE but we’ll record RMSE as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "609407cf-77b3-4c81-b7d7-38950af71695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF best params: {'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 137}\n",
      "RF best MAE (CV): 30.146859644296445\n"
     ]
    }
   ],
   "source": [
    "# 1. RandomForest tuning with RandomizedSearchCV\n",
    "rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 200),    # sample between 50 and 200\n",
    "    'max_depth': [None] + list(range(5, 21, 5)),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "\n",
    "rs_rf = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=20,                         # only 20 parameter settings\n",
    "    scoring=mae_scorer,\n",
    "    cv=GroupKFold(n_splits=3),\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "rs_rf.fit(X_train, y_train, groups=groups)\n",
    "print(\"RF best params:\", rs_rf.best_params_)\n",
    "print(\"RF best MAE (CV):\", -rs_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0121496-edcc-41b9-a5ae-2dcab72e7e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-12 20:42:03,628] A new study created in memory with name: no-name-7d0cba5c-788d-4fe2-a5fb-5de81c4e7c5c\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573e00e0ec9546618702899858ceb7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laptop 1\\AppData\\Local\\Temp\\ipykernel_16952\\3613777901.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n",
      "C:\\Users\\laptop 1\\AppData\\Local\\Temp\\ipykernel_16952\\3613777901.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
      "C:\\Users\\laptop 1\\AppData\\Local\\Temp\\ipykernel_16952\\3613777901.py:13: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-07-12 20:42:03,673] Trial 0 failed with parameters: {'learning_rate': 0.015355286838886862, 'num_leaves': 97, 'min_data_in_leaf': 40, 'feature_fraction': 0.8394633936788146, 'bagging_fraction': 0.6624074561769746, 'bagging_freq': 1} because of the following error: TypeError(\"cv() got an unexpected keyword argument 'early_stopping_rounds'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\laptop 1\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\laptop 1\\AppData\\Local\\Temp\\ipykernel_16952\\3613777901.py\", line 16, in objective\n",
      "    cv_results = lgb.cv(\n",
      "                 ^^^^^^^\n",
      "TypeError: cv() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "[W 2025-07-12 20:42:03,678] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cv() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1-mean\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mRANDOM_STATE))\n\u001b[1;32m---> 31\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBM best params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBM best MAE (CV):\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     _optimize(\n\u001b[0;32m    490\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    491\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    492\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    493\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    494\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    495\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    496\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    497\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    498\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    499\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         _optimize_sequential(\n\u001b[0;32m     65\u001b[0m             study,\n\u001b[0;32m     66\u001b[0m             func,\n\u001b[0;32m     67\u001b[0m             n_trials,\n\u001b[0;32m     68\u001b[0m             timeout,\n\u001b[0;32m     69\u001b[0m             catch,\n\u001b[0;32m     70\u001b[0m             callbacks,\n\u001b[0;32m     71\u001b[0m             gc_after_trial,\n\u001b[0;32m     72\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     74\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     75\u001b[0m         )\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[0;32m      3\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagging_freq\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagging_freq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     15\u001b[0m     }\n\u001b[1;32m---> 16\u001b[0m     cv_results \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mcv(\n\u001b[0;32m     17\u001b[0m         params,\n\u001b[0;32m     18\u001b[0m         lgb\u001b[38;5;241m.\u001b[39mDataset(X_train, label\u001b[38;5;241m=\u001b[39my_train),\n\u001b[0;32m     19\u001b[0m         folds\u001b[38;5;241m=\u001b[39mGroupKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(X_train, y_train, groups),\n\u001b[0;32m     20\u001b[0m         num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m     21\u001b[0m         early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     22\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     23\u001b[0m         seed\u001b[38;5;241m=\u001b[39mRANDOM_STATE,\n\u001b[0;32m     24\u001b[0m         stratified\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m         verbose_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# return the last CV MAE\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1-mean\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: cv() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "# 2. LightGBM tuning with Optuna + early stopping\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'l1',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 5)\n",
    "    }\n",
    "    cv_results = lgb.cv(\n",
    "        params,\n",
    "        lgb.Dataset(X_train, label=y_train),\n",
    "        folds=GroupKFold(n_splits=3).split(X_train, y_train, groups),\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=20,\n",
    "        metrics=['l1'],\n",
    "        seed=RANDOM_STATE,\n",
    "        stratified=False,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    # return the last CV MAE\n",
    "    return cv_results['l1-mean'][-1]\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(\"LGBM best params:\", study.best_params)\n",
    "print(\"LGBM best MAE (CV):\", study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
